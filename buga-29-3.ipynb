{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eli - basic_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elisim/DeepTIME-Datahack2019/blob/master/buga-29-3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhoQ0WE77laV",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzcHApBwwSHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROpca4B6uS58",
        "colab_type": "text"
      },
      "source": [
        "# Orcam code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqu14fPUf00x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9QIy2-Xw4Ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import numpy as np\n",
        "\n",
        "FSENCODING = sys.getfilesystemencoding()\n",
        "\n",
        "\n",
        "def enumerate_paths(paths):\n",
        "    # Extract sequences/videos/people from the frame-paths\n",
        "    sequences = [os.path.dirname(p) for p in paths]\n",
        "    videos = [os.path.dirname(s) for s in sequences]\n",
        "    people = [os.path.dirname(c) for c in videos]\n",
        "\n",
        "    # Enumerate the frames based on videos and people\n",
        "    unique_videos, video_ids = np.unique(videos, return_inverse=True)\n",
        "    unique_people, person_ids = np.unique(people, return_inverse=True)\n",
        "    return person_ids, video_ids\n",
        "\n",
        "\n",
        "def split_by(data, indices):\n",
        "    # Split data based on a numpy array of sorted indices\n",
        "    sections = np.where(np.diff(indices))[0] + 1\n",
        "    split_data = np.split(data, sections)\n",
        "    return split_data\n",
        "\n",
        "\n",
        "def parse_tarinfo(buff):\n",
        "    # Get a version-compatible tarinfo parser\n",
        "    if not hasattr(parse_tarinfo, 'defaultargs'):\n",
        "        # Determine version once on first call\n",
        "        dummy_header = tarfile.TarInfo().tobuf()\n",
        "        try:\n",
        "            _ = tarfile.TarInfo.frombuf(dummy_header)\n",
        "            parse_tarinfo.defaultargs = False\n",
        "        except TypeError:\n",
        "            parse_tarinfo.defaultargs = True\n",
        "    if parse_tarinfo.defaultargs:\n",
        "        # Python 3\n",
        "        return tarfile.TarInfo.frombuf(buff, FSENCODING, 'surrogateescape')\n",
        "    else:\n",
        "        # Python 2\n",
        "        return tarfile.TarInfo.frombuf(buff)\n",
        "      \n",
        "      \n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "\n",
        "\n",
        "class Images(object):\n",
        "    # A class for easy and fast reading of images packed in a tar file\n",
        "    def __init__(self, path, index_path=None):\n",
        "        self.path = path\n",
        "        if index_path is None:\n",
        "            # index file is the same as tar path but  .pkl\n",
        "            index_path = path[:-3] + 'pkl'\n",
        "        if not os.path.exists(index_path):\n",
        "            print('Indexing tar file, this could take a few minutes...')\n",
        "            self._tar_index = self._index_tar(path)\n",
        "            print('done')\n",
        "            # Save index file\n",
        "            with open(index_path, 'wb') as fid:\n",
        "                pkl.dump(self._tar_index, fid)\n",
        "        else:\n",
        "            with open(index_path, 'rb') as fid:\n",
        "                self._tar_index = pkl.load(fid)\n",
        "        self.index_path = index_path\n",
        "        # Open the tar file\n",
        "        self.fid = open(path, 'rb')\n",
        "        # Get its size for later checking the indexing validity\n",
        "        self.fid.seek(0, 2)\n",
        "        self.tar_size = self.fid.tell()\n",
        "        # save a sorted list of the tar file paths (keys)\n",
        "        self.keys = sorted(self._tar_index.keys())\n",
        "\n",
        "    @staticmethod\n",
        "    def _index_tar(path):\n",
        "        # Build a dictionary with the locations of all data points\n",
        "        tar_index = {}\n",
        "        with tarfile.TarFile(path, \"r\") as tar:\n",
        "            for tarinfo in tar:\n",
        "                if tarinfo.isfile():\n",
        "                    offsets_and_size = (\n",
        "                        tarinfo.offset, tarinfo.offset_data, tarinfo.size)\n",
        "                    tar_index[tarinfo.name] = offsets_and_size\n",
        "        return tar_index\n",
        "\n",
        "    @staticmethod\n",
        "    def _decode_image(buff):\n",
        "        # Decode an image buffer from memory\n",
        "        buff_array = np.asarray(bytearray(buff), dtype='uint8')\n",
        "        image = cv2.imdecode(buff_array, cv2.IMREAD_UNCHANGED)\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._tar_index)\n",
        "\n",
        "    @property\n",
        "    def paths(self):\n",
        "        return self.keys\n",
        "\n",
        "    def _getitem(self, item):\n",
        "        # A private _getitem for better readability\n",
        "        # If item is an index, replace with the path at that index\n",
        "        if isinstance(item, int):\n",
        "            item = self.keys[item]\n",
        "        # Grab an image buffer based on its path and decode it\n",
        "        offset, data_offset, size = self._tar_index[item]\n",
        "        # Go to start of record\n",
        "        self.fid.seek(offset)\n",
        "        # Check indexing validty\n",
        "        header_size = data_offset - offset  # should always be 512\n",
        "        tarinfo = parse_tarinfo(self.fid.read(header_size))\n",
        "        if tarinfo.path != item:\n",
        "            raise tarfile.InvalidHeaderError\n",
        "        buff = self.fid.read(size)\n",
        "        image = self._decode_image(buff)[:, :, ::-1]\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        try:\n",
        "            image = self._getitem(item)\n",
        "        except (tarfile.InvalidHeaderError, tarfile.TruncatedHeaderError, tarfile.EmptyHeaderError):\n",
        "            error_str = 'Index file \"{}\" does not match tarfile \"{}\". Remove the index file and try again.'\n",
        "            raise IOError(error_str.format(self.index_path, self.path))\n",
        "\n",
        "        return image\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, type, value, tb):\n",
        "        self.fid.close()\n",
        "\n",
        "\n",
        "def compatible_load(path):\n",
        "    # pickle loading compatible for pyton 2/3\n",
        "    data = None\n",
        "    with open(path, 'rb') as fid:\n",
        "        try:\n",
        "            data = pkl.load(fid)\n",
        "        except UnicodeDecodeError:\n",
        "            # Python 3 compatability\n",
        "            fid.seek(0)\n",
        "            data = pkl.load(fid, encoding='latin1')\n",
        "    return data\n",
        "\n",
        "\n",
        "def read_pose(pose_path):\n",
        "    # Read the pose points from file\n",
        "    data = compatible_load(pose_path)\n",
        "    keypoints = data['keypoints']\n",
        "    scores = data['scores']\n",
        "    paths = data['paths']\n",
        "    return paths, keypoints, scores\n",
        "\n",
        "\n",
        "def read_signatures(sigs_path):\n",
        "    # Read the imagenet signatures from file\n",
        "    data = compatible_load(sigs_path)\n",
        "    signatures = data['signatures']\n",
        "    paths = data['paths']\n",
        "    return paths, signatures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "553aGqEqzTPD",
        "colab_type": "text"
      },
      "source": [
        "# Our code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-4uLPBm4t70",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thaOasH7xBI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sigs_paths , sigs = read_signatures('./drive/My Drive/DataHack-Storage/signatures.pkl')\n",
        "pose_paths, keypoints, scores = read_pose('./drive/My Drive/DataHack-Storage/pose.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57EHdfSvUPfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from more_itertools import unzip\n",
        "zipped_key_poses =  list(zip(pose_paths, keypoints, scores))\n",
        "pose_paths, keypoints = unzip([(x  , np.multiply(y.T , ((np.array(z)>0)*2-1)).T)  for x,y,z in zipped_key_poses])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mrtaRjCo424",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zipp_by_path(*dicts):\n",
        "  reses = [dk for dk in dicts]\n",
        "  keys = reses[0].keys()\n",
        "  print(len(keys))\n",
        "  for r in reses[1:]:\n",
        "    keys_set = set(keys)\n",
        "    keys = [x for x in r if x in keys_set]\n",
        "    print(len(keys))\n",
        "  \n",
        "  return [\n",
        "      [key] + [r[key] for r in reses] for key in keys\n",
        "  ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p83TiYa0uxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Split train & test signs\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "person_ids, video_ids = enumerate_paths(sigs_paths)\n",
        "unique_person_ids = np.unique(person_ids)\n",
        "unique_video_ids = np.unique(video_ids)\n",
        "\n",
        "\n",
        "zipped = zip(person_ids , video_ids , sigs , sigs_paths)\n",
        "zipped = [x for x in zipped]\n",
        "\n",
        "\n",
        "   \n",
        "def cartesian_product(grp):\n",
        "  x =  cosine_similarity(grp , grp).flatten()\n",
        "  return x * (x > 0)\n",
        "#   return [x for x in [cosine_similarity([i] , [j]) for i in grp for j in grp] if x != 1]\n",
        "\n",
        "\n",
        "def zip_to_dict(xs,ys):\n",
        "  return {x:y for x,y in zip(xs , ys) }\n",
        "  \n",
        "zipped_res = zipp_by_path(\n",
        "    zip_to_dict(sigs_paths , person_ids),\n",
        "    zip_to_dict(sigs_paths , sigs) , \n",
        "    zip_to_dict(pose_paths, [cartesian_product(ps) for ps in keypoints])\n",
        ") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkYqh6zTshG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract(data):\n",
        "  return [x[1] for x in data] , [np.concatenate((x[2] , x[3]))  for x in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTqHUSHNtiCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ids , sigs = extract(zipped_res)\n",
        "\n",
        "ids = np.array(ids)\n",
        "sigs = np.array(sigs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcC7oVZAP3aB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ids , sigs = extract(zipped_res)\n",
        "\n",
        "# ids = np.array(ids)\n",
        "# sigs = np.array(sigs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHWiKdox4y6_",
        "colab_type": "text"
      },
      "source": [
        "## Model Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ODch-OFCaW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import regularizers\n",
        "from keras.callbacks import EarlyStopping , ModelCheckpoint , ReduceLROnPlateau\n",
        "\n",
        "earlystopper = EarlyStopping(monitor='acc',patience=4, verbose=10)\n",
        "checkpointer = ModelCheckpoint(monitor='acc',filepath = 'model_zero7.{epoch:02d}-{loss:.6f}.hdf5',verbose=10,save_best_only=True, save_weights_only = True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='acc', factor=0.2,patience=2, min_lr=0.000001, verbose=10)\n",
        "\n",
        "cbs = [earlystopper , checkpointer , reduce_lr]\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense( 512 ,activation=tf.nn.relu , input_shape=(2337,)),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(512, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.05)),\n",
        "    keras.layers.Dense(101, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhan11blCaW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os \n",
        "# os.environ['TF_KERAS'] = '1'\n",
        "\n",
        "# from keras_radam import RAdam\n",
        "# model.compile(optimizer=RAdam(),\n",
        "#               loss='sparse_categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWfviHN88q-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install keras-radam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE0yJgpMbZp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit(sigs_train, id_train, epochs=25)\n",
        "model.fit(sigs, ids, epochs=14 , callbacks = cbs )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EELijBJ8ueOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(sigs, ids, epochs=1 , callbacks = cbs )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsoS7CPDCaXH",
        "colab_type": "text"
      },
      "source": [
        "# Test Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd7Pgsu6CaXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paths_eva , sigs_eva = read_signatures('./drive/My Drive/DataHack-Storage/sig-test-new.pkl')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz1YRnPfqNtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paths_pos_eva , pos_eva , score_eva = read_pose('./drive/My Drive/DataHack-Storage/pose_test.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI7QyKlNbwTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zipped_key_poses_eva =  list(zip(paths_pos_eva, pos_eva, score_eva))\n",
        "from more_itertools import unzip\n",
        "not_unzipped = [(x  , np.multiply(y.T , ((np.array(z)>0)*2-1)).T)  for x,y,z in zipped_key_poses_eva]\n",
        "paths_pos_eva = [x[0] for x in not_unzipped] \n",
        "pos_eva = [x[1] for x in not_unzipped]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwTH7Z1DwAgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def enumerate_paths_eva(paths):\n",
        "    # Extract sequences/videos/people from the frame-paths\n",
        "    sequences = [os.path.dirname(p) for p in paths]\n",
        "    return sequences\n",
        "\n",
        "\n",
        "zipped_res = zipp_by_path(\n",
        "    zip_to_dict(paths_eva , enumerate_paths_eva(paths_eva)) ,\n",
        "    zip_to_dict(paths_eva , sigs_eva) , \n",
        "    zip_to_dict(paths_pos_eva, [cartesian_product(ps) for ps in pos_eva])\n",
        ") \n",
        "\n",
        "def extract_eva(data):\n",
        "  return  [ x[1] for x in data ], [np.concatenate((x[2] , x[3])) for x in data]\n",
        "\n",
        "seqs_eva , sigs_concat_pos_eva = extract_eva(zipped_res)\n",
        "sigs_concat_pos_eva = np.array(sigs_concat_pos_eva)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3LFv7kl-tlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def enumerate_paths_eva(paths):\n",
        "    # Extract sequences/videos/people from the frame-paths\n",
        "    sequences = [os.path.dirname(p) for p in paths]\n",
        "    return sequences\n",
        "  \n",
        "seqs_eva = enumerate_paths_eva(paths_eva)\n",
        "zipped_eva = [x for x in zip(seqs_eva , sigs_eva)]\n",
        "evaluations = model.predict(sigs_concat_pos_eva)\n",
        "evaluations.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5IwXBLDVhoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## build dict of seq -> 101 vector of predictions \n",
        "from collections import defaultdict\n",
        "\n",
        "res = {}\n",
        "for seq_name, pred in zip(seqs_eva,evaluations):\n",
        "  if not seq_name in res:\n",
        "    res[seq_name] = pred\n",
        "  else:\n",
        "    res[seq_name] = np.add(pred ,res[seq_name])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWePGyi0dwpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_res= {} # dict of seq -> 5 dim vector\n",
        "for a,b in res.items():\n",
        "  x = np.flip(np.argsort(b))\n",
        "  top_5 = [int(i) for i in x[:5]]\n",
        "  final_res[a] = top_5 \n",
        "\n",
        "submissions = [final_res[x]  for x in  final_res]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rxS5Is6TE52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from urllib.request import urlopen\n",
        "    from urllib.request import Request\n",
        "except ImportError:\n",
        "    from urllib2 import urlopen\n",
        "    from urllib2 import Request\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def submit(name, submission):\n",
        "    # Submit your result to the leaderboard\n",
        "    jsonStr = json.dumps({'submitter': name, 'predictions': submission})\n",
        "    data = jsonStr.encode('utf-8')\n",
        "    req = Request('https://leaderboard.datahack.org.il/orcam/api',\n",
        "                  headers={'Content-Type': 'application/json'},\n",
        "                  data=data)\n",
        "    resp = urlopen(req)\n",
        "    jsresp = json.load(resp)\n",
        "    return jsresp\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFz1NX61TG1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = submit('Sigs are what?' , submissions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZKIwMhd4GIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v45xRNDG1LZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "subdir = './drive/My Drive/DataHack-Storage/submissions'\n",
        "def save_submission(submissions, result):\n",
        "  submission_files = os.listdir(subdir)\n",
        "  max_count = 0\n",
        "  fname = '{}/sub_{}_score_{}.json'\n",
        "  for f in submission_files:\n",
        "    pref = f.index('sub_')\n",
        "    end = f.index('_score_', pref)\n",
        "    count = f[pref+ len('sub_'):end]\n",
        "    count = int(count)\n",
        "    if count > max_count:\n",
        "      max_count = count\n",
        "  max_count += 1\n",
        "  score = result['score']\n",
        "  fname = fname.format(subdir, max_count, score)\n",
        "  with open(fname, 'w') as f:\n",
        "    f.write(json.dumps(submissions))\n",
        "def load_best_submission():\n",
        "  submission_files = os.listdir(subdir)\n",
        "  max_score, max_submission = 0, None\n",
        "  for f in submission_files:\n",
        "    pref = f.index('_score_')\n",
        "    end = f.index('.json', pref)\n",
        "    score = f[pref+len('_score_')+1:end]\n",
        "    score = float(score)\n",
        "    if score > max_score:\n",
        "      max_score = score\n",
        "      max_submission = f\n",
        "  actual_submission = []\n",
        "  with open(max_submission, 'r') as file:\n",
        "    actual_submission = json.loads(file.read())\n",
        "  return actual_submission\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo7s0BRX1Umk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_submission(submissions , results)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}