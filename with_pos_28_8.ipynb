{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eli - basic_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elisim/DeepTIME-Datahack2019/blob/master/with_pos_28_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhoQ0WE77laV",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzcHApBwwSHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROpca4B6uS58",
        "colab_type": "text"
      },
      "source": [
        "# Orcam code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqu14fPUf00x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9QIy2-Xw4Ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import numpy as np\n",
        "\n",
        "FSENCODING = sys.getfilesystemencoding()\n",
        "\n",
        "\n",
        "def enumerate_paths(paths):\n",
        "    # Extract sequences/videos/people from the frame-paths\n",
        "    sequences = [os.path.dirname(p) for p in paths]\n",
        "    videos = [os.path.dirname(s) for s in sequences]\n",
        "    people = [os.path.dirname(c) for c in videos]\n",
        "\n",
        "    # Enumerate the frames based on videos and people\n",
        "    unique_videos, video_ids = np.unique(videos, return_inverse=True)\n",
        "    unique_people, person_ids = np.unique(people, return_inverse=True)\n",
        "    return person_ids, video_ids\n",
        "\n",
        "\n",
        "def split_by(data, indices):\n",
        "    # Split data based on a numpy array of sorted indices\n",
        "    sections = np.where(np.diff(indices))[0] + 1\n",
        "    split_data = np.split(data, sections)\n",
        "    return split_data\n",
        "\n",
        "\n",
        "def parse_tarinfo(buff):\n",
        "    # Get a version-compatible tarinfo parser\n",
        "    if not hasattr(parse_tarinfo, 'defaultargs'):\n",
        "        # Determine version once on first call\n",
        "        dummy_header = tarfile.TarInfo().tobuf()\n",
        "        try:\n",
        "            _ = tarfile.TarInfo.frombuf(dummy_header)\n",
        "            parse_tarinfo.defaultargs = False\n",
        "        except TypeError:\n",
        "            parse_tarinfo.defaultargs = True\n",
        "    if parse_tarinfo.defaultargs:\n",
        "        # Python 3\n",
        "        return tarfile.TarInfo.frombuf(buff, FSENCODING, 'surrogateescape')\n",
        "    else:\n",
        "        # Python 2\n",
        "        return tarfile.TarInfo.frombuf(buff)\n",
        "      \n",
        "      \n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "\n",
        "\n",
        "class Images(object):\n",
        "    # A class for easy and fast reading of images packed in a tar file\n",
        "    def __init__(self, path, index_path=None):\n",
        "        self.path = path\n",
        "        if index_path is None:\n",
        "            # index file is the same as tar path but  .pkl\n",
        "            index_path = path[:-3] + 'pkl'\n",
        "        if not os.path.exists(index_path):\n",
        "            print('Indexing tar file, this could take a few minutes...')\n",
        "            self._tar_index = self._index_tar(path)\n",
        "            print('done')\n",
        "            # Save index file\n",
        "            with open(index_path, 'wb') as fid:\n",
        "                pkl.dump(self._tar_index, fid)\n",
        "        else:\n",
        "            with open(index_path, 'rb') as fid:\n",
        "                self._tar_index = pkl.load(fid)\n",
        "        self.index_path = index_path\n",
        "        # Open the tar file\n",
        "        self.fid = open(path, 'rb')\n",
        "        # Get its size for later checking the indexing validity\n",
        "        self.fid.seek(0, 2)\n",
        "        self.tar_size = self.fid.tell()\n",
        "        # save a sorted list of the tar file paths (keys)\n",
        "        self.keys = sorted(self._tar_index.keys())\n",
        "\n",
        "    @staticmethod\n",
        "    def _index_tar(path):\n",
        "        # Build a dictionary with the locations of all data points\n",
        "        tar_index = {}\n",
        "        with tarfile.TarFile(path, \"r\") as tar:\n",
        "            for tarinfo in tar:\n",
        "                if tarinfo.isfile():\n",
        "                    offsets_and_size = (\n",
        "                        tarinfo.offset, tarinfo.offset_data, tarinfo.size)\n",
        "                    tar_index[tarinfo.name] = offsets_and_size\n",
        "        return tar_index\n",
        "\n",
        "    @staticmethod\n",
        "    def _decode_image(buff):\n",
        "        # Decode an image buffer from memory\n",
        "        buff_array = np.asarray(bytearray(buff), dtype='uint8')\n",
        "        image = cv2.imdecode(buff_array, cv2.IMREAD_UNCHANGED)\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._tar_index)\n",
        "\n",
        "    @property\n",
        "    def paths(self):\n",
        "        return self.keys\n",
        "\n",
        "    def _getitem(self, item):\n",
        "        # A private _getitem for better readability\n",
        "        # If item is an index, replace with the path at that index\n",
        "        if isinstance(item, int):\n",
        "            item = self.keys[item]\n",
        "        # Grab an image buffer based on its path and decode it\n",
        "        offset, data_offset, size = self._tar_index[item]\n",
        "        # Go to start of record\n",
        "        self.fid.seek(offset)\n",
        "        # Check indexing validty\n",
        "        header_size = data_offset - offset  # should always be 512\n",
        "        tarinfo = parse_tarinfo(self.fid.read(header_size))\n",
        "        if tarinfo.path != item:\n",
        "            raise tarfile.InvalidHeaderError\n",
        "        buff = self.fid.read(size)\n",
        "        image = self._decode_image(buff)[:, :, ::-1]\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        try:\n",
        "            image = self._getitem(item)\n",
        "        except (tarfile.InvalidHeaderError, tarfile.TruncatedHeaderError, tarfile.EmptyHeaderError):\n",
        "            error_str = 'Index file \"{}\" does not match tarfile \"{}\". Remove the index file and try again.'\n",
        "            raise IOError(error_str.format(self.index_path, self.path))\n",
        "\n",
        "        return image\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, type, value, tb):\n",
        "        self.fid.close()\n",
        "\n",
        "\n",
        "def compatible_load(path):\n",
        "    # pickle loading compatible for pyton 2/3\n",
        "    data = None\n",
        "    with open(path, 'rb') as fid:\n",
        "        try:\n",
        "            data = pkl.load(fid)\n",
        "        except UnicodeDecodeError:\n",
        "            # Python 3 compatability\n",
        "            fid.seek(0)\n",
        "            data = pkl.load(fid, encoding='latin1')\n",
        "    return data\n",
        "\n",
        "\n",
        "def read_pose(pose_path):\n",
        "    # Read the pose points from file\n",
        "    data = compatible_load(pose_path)\n",
        "    keypoints = data['keypoints']\n",
        "    scores = data['scores']\n",
        "    paths = data['paths']\n",
        "    return paths, keypoints, scores\n",
        "\n",
        "\n",
        "def read_signatures(sigs_path):\n",
        "    # Read the imagenet signatures from file\n",
        "    data = compatible_load(sigs_path)\n",
        "    signatures = data['signatures']\n",
        "    paths = data['paths']\n",
        "    return paths, signatures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "553aGqEqzTPD",
        "colab_type": "text"
      },
      "source": [
        "# Our code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-4uLPBm4t70",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thaOasH7xBI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sigs_paths , sigs = read_signatures('./drive/My Drive/DataHack-Storage/signatures.pkl')\n",
        "pose_paths, keypoints, scores = read_pose('./drive/My Drive/DataHack-Storage/pose.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p83TiYa0uxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Split train & test signs\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "person_ids, video_ids = enumerate_paths(sigs_paths)\n",
        "unique_person_ids = np.unique(person_ids)\n",
        "unique_video_ids = np.unique(video_ids)\n",
        "\n",
        "\n",
        "zipped = zip(person_ids , video_ids , sigs , sigs_paths)\n",
        "zipped = [x for x in zipped]\n",
        "\n",
        "def zipp_by_path(*dicts):\n",
        "  reses = [dk for dk in dicts]\n",
        "  keys = reses[0].keys()\n",
        "  for r in reses[1:]:\n",
        "    keys_set = set(keys)\n",
        "    keys = [x for x in r if x in keys_set]\n",
        "  \n",
        "  return [\n",
        "      [key] + [r[key] for r in reses] for key in keys\n",
        "  ]\n",
        "   \n",
        "def cartesian_product(grp):\n",
        "  return cosine_similarity(grp , grp).flatten()\n",
        "#   return [x for x in [cosine_similarity([i] , [j]) for i in grp for j in grp] if x != 1]\n",
        "\n",
        "\n",
        "def zip_to_dict(xs,ys):\n",
        "  return {x:y for x,y in zip(xs , ys) }\n",
        "  \n",
        "zipped_res = zipp_by_path(\n",
        "    zip_to_dict(sigs_paths , person_ids),\n",
        "    zip_to_dict(sigs_paths , sigs) , \n",
        "    zip_to_dict(pose_paths, [cartesian_product(ps) for ps in keypoints])\n",
        ") \n",
        "\n",
        "  \n",
        "# def concat_data(pose_paths , keypoints , sigs_paths , sigs):\n",
        "#   zipp_by_path(pose_creator , sigs_creator)\n",
        "  \n",
        "  \n",
        "#   sig_path_to_sig_zipped = {\n",
        "#       x[0]: x for x in sigs_zipped\n",
        "#   }\n",
        "\n",
        "#   zipped = {}\n",
        "  \n",
        "#   for pos_path , kps in zip(pose_paths , keypoints):\n",
        "#     if pos_path in path_to_zipped:\n",
        "#       x = path_to_zipped[pos_path]\n",
        "#       zipped_with_keypoints[pos_path] = (x[0] , x[1], x[2] , x[3] , cartesian_product(kps) )\n",
        "\n",
        "\n",
        "#   def extract_new_vec(zipped):\n",
        "#     return (zipped[0] ,zipped[1] , np.array(list(zipped[2]) + list(zipped[4])))  \n",
        "\n",
        "#   new_zipped = [ extract_new_vec(zipped_with_keypoints[key]) for key in zipped_with_keypoints ]\n",
        "#   return new_zipped\n",
        "\n",
        "# zipped = [x for x in new_zipped]\n",
        "    \n",
        "# videos_train ,videos_test = train_test_split(unique_video_ids , test_size = 0.1 , random_state = 42)\n",
        "\n",
        "# zipped_train = [x for x in zipped if x[1] in videos_train]\n",
        "# zipped_test = [x for x in zipped if x[1] in videos_test]\n",
        "\n",
        "\n",
        "\n",
        "# sigs_train = np.array([x[2] for x in zipped_train])\n",
        "# sigs_test = np.array([x[2] for x in zipped_test])\n",
        "# id_train = np.array([x[0] for x in zipped_train])\n",
        "# id_test = np.array([x[0] for x in zipped_test])\n",
        "\n",
        "# print(\"sigs_test.shape= {}\".format(sigs_test.shape))\n",
        "# print(\"sigs_train.shape= {}\".format(sigs_train.shape))\n",
        "# print(\"id_train.shape= {}\".format(id_train.shape))\n",
        "# print(\"id_test.shape= {}\".format(id_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkYqh6zTshG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract(data):\n",
        "  return [x[1] for x in data] , [np.concatenate((x[2] , x[3]))  for x in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTqHUSHNtiCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ids , sigs = extract(zipped_res)\n",
        "\n",
        "ids = np.array(ids)\n",
        "sigs = np.array(sigs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z90wqUTge_bM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "new_sigs = np.array([x[2] for x in zipped])\n",
        "new_ids = np.array([x[0] for x in zipped])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHWiKdox4y6_",
        "colab_type": "text"
      },
      "source": [
        "## Model Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ODch-OFCaW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(512 ,activation=tf.nn.relu , input_shape=(2337,)),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(512, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.05)),\n",
        "    keras.layers.Dense(101, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhan11blCaW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## RAdam\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE0yJgpMbZp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit(sigs_train, id_train, epochs=25)\n",
        "model.fit(sigs, ids, epochs=40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsoS7CPDCaXH",
        "colab_type": "text"
      },
      "source": [
        "# Test Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd7Pgsu6CaXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paths_eva , sigs_eva = read_signatures('./drive/My Drive/DataHack-Storage/sig-test-new.pkl')\n",
        "paths_pos_eva , pos_eva , _ = read_pose('./drive/My Drive/DataHack-Storage/pose_test.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwTH7Z1DwAgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def enumerate_paths_eva(paths):\n",
        "    # Extract sequences/videos/people from the frame-paths\n",
        "    sequences = [os.path.dirname(p) for p in paths]\n",
        "    return sequences\n",
        "\n",
        "\n",
        "zipped_res = zipp_by_path(\n",
        "    zip_to_dict(paths_eva , enumerate_paths_eva(paths_eva)) ,\n",
        "    zip_to_dict(paths_eva , sigs_eva) , \n",
        "    zip_to_dict(paths_pos_eva, [cartesian_product(ps) for ps in pos_eva])\n",
        ") \n",
        "\n",
        "def extract_eva(data):\n",
        "  return  [ x[1] for x in data ], [np.concatenate((x[2] , x[3])) for x in data]\n",
        "\n",
        "seqs_eva , sigs_concat_pos_eva = extract_eva(zipped_res)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhmkjxCqzODG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sigs_concat_pos_eva = np.array(sigs_concat_pos_eva)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3LFv7kl-tlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def enumerate_paths_eva(paths):\n",
        "    # Extract sequences/videos/people from the frame-paths\n",
        "    sequences = [os.path.dirname(p) for p in paths]\n",
        "    return sequences\n",
        "  \n",
        "seqs_eva = enumerate_paths_eva(paths_eva)\n",
        "zipped_eva = [x for x in zip(seqs_eva , sigs_eva)]\n",
        "evaluations = model.predict(sigs_concat_pos_eva)\n",
        "evaluations.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5IwXBLDVhoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## build dict of seq -> 101 vector of predictions \n",
        "from collections import defaultdict\n",
        "\n",
        "res = {}\n",
        "for seq_name, pred in zip(seqs_eva,evaluations):\n",
        "  if not seq_name in res:\n",
        "    res[seq_name] = pred\n",
        "  else:\n",
        "    res[seq_name] = np.add(pred ,res[seq_name])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWePGyi0dwpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_res= {} # dict of seq -> 5 dim vector\n",
        "for a,b in res.items():\n",
        "  x = np.flip(np.argsort(b))\n",
        "  top_5 = [int(i) for i in x[:5]]\n",
        "  final_res[a] = top_5 \n",
        "\n",
        "submissions = [final_res[x]  for x in  final_res]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rxS5Is6TE52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from urllib.request import urlopen\n",
        "    from urllib.request import Request\n",
        "except ImportError:\n",
        "    from urllib2 import urlopen\n",
        "    from urllib2 import Request\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def submit(name, submission):\n",
        "    # Submit your result to the leaderboard\n",
        "    jsonStr = json.dumps({'submitter': name, 'predictions': submission})\n",
        "    data = jsonStr.encode('utf-8')\n",
        "    req = Request('https://leaderboard.datahack.org.il/orcam/api',\n",
        "                  headers={'Content-Type': 'application/json'},\n",
        "                  data=data)\n",
        "    resp = urlopen(req)\n",
        "    print(json.load(resp))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFz1NX61TG1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit('DeepTIME' , submissions)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}