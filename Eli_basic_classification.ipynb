{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eli - basic_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elisim/DeepTIME-Datahack2019/blob/master/Eli_basic_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhoQ0WE77laV",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzcHApBwwSHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROpca4B6uS58",
        "colab_type": "text"
      },
      "source": [
        "# Orcam code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9QIy2-Xw4Ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import numpy as np\n",
        "\n",
        "FSENCODING = sys.getfilesystemencoding()\n",
        "\n",
        "\n",
        "def enumerate_paths(paths):\n",
        "    # Extract sequences/videos/people from the frame-paths\n",
        "    sequences = [os.path.dirname(p) for p in paths]\n",
        "    videos = [os.path.dirname(s) for s in sequences]\n",
        "    people = [os.path.dirname(c) for c in videos]\n",
        "\n",
        "    # Enumerate the frames based on videos and people\n",
        "    unique_videos, video_ids = np.unique(videos, return_inverse=True)\n",
        "    unique_people, person_ids = np.unique(people, return_inverse=True)\n",
        "    return person_ids, video_ids\n",
        "\n",
        "\n",
        "def split_by(data, indices):\n",
        "    # Split data based on a numpy array of sorted indices\n",
        "    sections = np.where(np.diff(indices))[0] + 1\n",
        "    split_data = np.split(data, sections)\n",
        "    return split_data\n",
        "\n",
        "\n",
        "def parse_tarinfo(buff):\n",
        "    # Get a version-compatible tarinfo parser\n",
        "    if not hasattr(parse_tarinfo, 'defaultargs'):\n",
        "        # Determine version once on first call\n",
        "        dummy_header = tarfile.TarInfo().tobuf()\n",
        "        try:\n",
        "            _ = tarfile.TarInfo.frombuf(dummy_header)\n",
        "            parse_tarinfo.defaultargs = False\n",
        "        except TypeError:\n",
        "            parse_tarinfo.defaultargs = True\n",
        "    if parse_tarinfo.defaultargs:\n",
        "        # Python 3\n",
        "        return tarfile.TarInfo.frombuf(buff, FSENCODING, 'surrogateescape')\n",
        "    else:\n",
        "        # Python 2\n",
        "        return tarfile.TarInfo.frombuf(buff)\n",
        "      \n",
        "      \n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "\n",
        "\n",
        "class Images(object):\n",
        "    # A class for easy and fast reading of images packed in a tar file\n",
        "    def __init__(self, path, index_path=None):\n",
        "        self.path = path\n",
        "        if index_path is None:\n",
        "            # index file is the same as tar path but  .pkl\n",
        "            index_path = path[:-3] + 'pkl'\n",
        "        if not os.path.exists(index_path):\n",
        "            print('Indexing tar file, this could take a few minutes...')\n",
        "            self._tar_index = self._index_tar(path)\n",
        "            print('done')\n",
        "            # Save index file\n",
        "            with open(index_path, 'wb') as fid:\n",
        "                pkl.dump(self._tar_index, fid)\n",
        "        else:\n",
        "            with open(index_path, 'rb') as fid:\n",
        "                self._tar_index = pkl.load(fid)\n",
        "        self.index_path = index_path\n",
        "        # Open the tar file\n",
        "        self.fid = open(path, 'rb')\n",
        "        # Get its size for later checking the indexing validity\n",
        "        self.fid.seek(0, 2)\n",
        "        self.tar_size = self.fid.tell()\n",
        "        # save a sorted list of the tar file paths (keys)\n",
        "        self.keys = sorted(self._tar_index.keys())\n",
        "\n",
        "    @staticmethod\n",
        "    def _index_tar(path):\n",
        "        # Build a dictionary with the locations of all data points\n",
        "        tar_index = {}\n",
        "        with tarfile.TarFile(path, \"r\") as tar:\n",
        "            for tarinfo in tar:\n",
        "                if tarinfo.isfile():\n",
        "                    offsets_and_size = (\n",
        "                        tarinfo.offset, tarinfo.offset_data, tarinfo.size)\n",
        "                    tar_index[tarinfo.name] = offsets_and_size\n",
        "        return tar_index\n",
        "\n",
        "    @staticmethod\n",
        "    def _decode_image(buff):\n",
        "        # Decode an image buffer from memory\n",
        "        buff_array = np.asarray(bytearray(buff), dtype='uint8')\n",
        "        image = cv2.imdecode(buff_array, cv2.IMREAD_UNCHANGED)\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._tar_index)\n",
        "\n",
        "    @property\n",
        "    def paths(self):\n",
        "        return self.keys\n",
        "\n",
        "    def _getitem(self, item):\n",
        "        # A private _getitem for better readability\n",
        "        # If item is an index, replace with the path at that index\n",
        "        if isinstance(item, int):\n",
        "            item = self.keys[item]\n",
        "        # Grab an image buffer based on its path and decode it\n",
        "        offset, data_offset, size = self._tar_index[item]\n",
        "        # Go to start of record\n",
        "        self.fid.seek(offset)\n",
        "        # Check indexing validty\n",
        "        header_size = data_offset - offset  # should always be 512\n",
        "        tarinfo = parse_tarinfo(self.fid.read(header_size))\n",
        "        if tarinfo.path != item:\n",
        "            raise tarfile.InvalidHeaderError\n",
        "        buff = self.fid.read(size)\n",
        "        image = self._decode_image(buff)[:, :, ::-1]\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        try:\n",
        "            image = self._getitem(item)\n",
        "        except (tarfile.InvalidHeaderError, tarfile.TruncatedHeaderError, tarfile.EmptyHeaderError):\n",
        "            error_str = 'Index file \"{}\" does not match tarfile \"{}\". Remove the index file and try again.'\n",
        "            raise IOError(error_str.format(self.index_path, self.path))\n",
        "\n",
        "        return image\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, type, value, tb):\n",
        "        self.fid.close()\n",
        "\n",
        "\n",
        "def compatible_load(path):\n",
        "    # pickle loading compatible for pyton 2/3\n",
        "    data = None\n",
        "    with open(path, 'rb') as fid:\n",
        "        try:\n",
        "            data = pkl.load(fid)\n",
        "        except UnicodeDecodeError:\n",
        "            # Python 3 compatability\n",
        "            fid.seek(0)\n",
        "            data = pkl.load(fid, encoding='latin1')\n",
        "    return data\n",
        "\n",
        "\n",
        "def read_pose(pose_path):\n",
        "    # Read the pose points from file\n",
        "    data = compatible_load(pose_path)\n",
        "    keypoints = data['keypoints']\n",
        "    scores = data['scores']\n",
        "    paths = data['paths']\n",
        "    return paths, keypoints, scores\n",
        "\n",
        "\n",
        "def read_signatures(sigs_path):\n",
        "    # Read the imagenet signatures from file\n",
        "    data = compatible_load(sigs_path)\n",
        "    signatures = data['signatures']\n",
        "    paths = data['paths']\n",
        "    return paths, signatures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "553aGqEqzTPD",
        "colab_type": "text"
      },
      "source": [
        "# Our code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-4uLPBm4t70",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thaOasH7xBI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sigs_paths , sigs = read_signatures('./drive/My Drive/DataHack-Storage/signatures.pkl')\n",
        "pose_paths, keypoints, scores = read_pose('./drive/My Drive/DataHack-Storage/pose.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p83TiYa0uxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Split train & test signs\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "person_ids, video_ids = enumerate_paths(sigs_paths)\n",
        "unique_person_ids = np.unique(person_ids)\n",
        "unique_video_ids = np.unique(video_ids)\n",
        "\n",
        "zipped = zip(person_ids , video_ids , sigs)\n",
        "zipped = [x for x in zipped]\n",
        "\n",
        "videos_train ,videos_test = train_test_split(unique_video_ids , test_size = 0.1 , random_state = 42)\n",
        "\n",
        "zipped_train = [x for x in zipped if x[1] in videos_train]\n",
        "zipped_test = [x for x in zipped if x[1] in videos_test]\n",
        "\n",
        "\n",
        "\n",
        "sigs_train = np.array([x[2] for x in zipped_train])\n",
        "sigs_test = np.array([x[2] for x in zipped_test])\n",
        "id_train = np.array([x[0] for x in zipped_train])\n",
        "id_test = np.array([x[0] for x in zipped_test])\n",
        "\n",
        "print(\"sigs_test.shape= {}\".format(sigs_test.shape))\n",
        "print(\"sigs_train.shape= {}\".format(sigs_train.shape))\n",
        "print(\"id_train.shape= {}\".format(id_train.shape))\n",
        "print(\"id_test.shape= {}\".format(id_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHWiKdox4y6_",
        "colab_type": "text"
      },
      "source": [
        "## Model Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ODch-OFCaW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(256 ,activation=tf.nn.relu , input_shape=(2048,)),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(256, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.05)),\n",
        "    keras.layers.Dense(101, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhan11blCaW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## RAdam\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE0yJgpMbZp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit(sigs_train, id_train, epochs=25)\n",
        "model.fit(sigs, person_ids, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsoS7CPDCaXH",
        "colab_type": "text"
      },
      "source": [
        "## Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd7Pgsu6CaXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paths_eva , sigs_eva = read_signatures('./drive/My Drive/DataHack-Storage/sig-test-new.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqfXd70BNR-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def enumerate_paths_eva(paths):\n",
        "    # Extract sequences/videos/people from the frame-paths\n",
        "    sequences = [os.path.dirname(p) for p in paths]\n",
        "    return sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYUYfdlhOaKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seqs_eva = enumerate_paths_eva(paths_eva)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvoP8EMVgBcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(list(set(seqs_eva)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzZeftwyOldi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zipped_eva = [x for x in zip(seqs_eva , sigs_eva)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdA-BnOBVK16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zipped_eva"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78GnSQInQbQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluations = model.predict(sigs_eva)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXGgw-NSQmrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluations.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5IwXBLDVhoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "res = {}\n",
        "for seq_name, ev in zip(seqs_eva,evaluations):\n",
        "  if not seq_name in res:\n",
        "    res[seq_name] = ev\n",
        "  else:\n",
        "    res[seq_name] = np.add(ev ,res[seq_name] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8d6xDNQfuBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWePGyi0dwpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_res= {}\n",
        "for a,b in res.items():\n",
        "  x = np.flip(np.argsort(b))\n",
        "  top_5 = [int(i) for i in x[:5]]\n",
        "  final_res[a] = top_5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjc_XALcep4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submissions = [ final_res[x]  for x in  final_res]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rxS5Is6TE52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from urllib.request import urlopen\n",
        "    from urllib.request import Request\n",
        "except ImportError:\n",
        "    from urllib2 import urlopen\n",
        "    from urllib2 import Request\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def submit(name, submission):\n",
        "    # Submit your result to the leaderboard\n",
        "    jsonStr = json.dumps({'submitter': name, 'predictions': submission})\n",
        "    data = jsonStr.encode('utf-8')\n",
        "    req = Request('https://leaderboard.datahack.org.il/orcam/api',\n",
        "                  headers={'Content-Type': 'application/json'},\n",
        "                  data=data)\n",
        "    resp = urlopen(req)\n",
        "    print(json.load(resp))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFz1NX61TG1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit('DeepTIME' , submissions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3vbsSeLmXLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}